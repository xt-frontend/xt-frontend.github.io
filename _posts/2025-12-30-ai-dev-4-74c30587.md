---
layout: post
title: "Vector Dimensions, Cosine Similarity, Dot Product — and Why Your Distance Metric Silently Ruins Relevance"
author: ai-assistant
categories: [AI Dev]
tags: [AI, 개발]
image: assets/images/ai-dev/ai-dev-4-74c30587.jpg
beforetoc: "AI 개발 트렌드 뉴스입니다. 원문: Dev.to"
toc: true
---

서론  
밀집 벡터가 의미 기반 검색의 핵심이라면, 거리 계산 방법은 "관련성"의 정의를 결정한다. 대부분의 RAG 시스템 실패는 임베딩 품질보다 **시각화되지 않은 유사도 선택**에서 비롯된다.  

본론  
1. **벡터 차원의 의미**  
   임베딩은 384~1536차원의 숫자 배열이지만, 각 차원은 단어가 아닌 **의미의 잠재 신호**를 나타낸다. 고차원은 세부 정보를 포착하지만 메모리 소비가 크고 해석이 어렵다. 저차원은 속도와 비용을 절감하지만 미묘한 차이를 잃는다. **차원 크기는 사용 사례에 따라 선택**해야 한다.  

2. **3가지 거리 메트릭**  
   - **코사인 유사도 (가장 일반적)**  
     벡터 간 **각도**를 측정해 방향이 일치하는지 확인. 크기와 무관하며, 길이가 다른 텍스트 간 비교에 안정적. 자연어 질문, RAG 시스템 등에서 효과적.  
   - **도트 프로덕트 (오용되기 쉬움)**  
     방향과 **크기**를 모두 고려. 긴 문서일수록 점수가 커져, 길이에 민감한 시스템(예: 문서 검색)에 적합.  
   - **유클리드 거리**  
     벡터 간 **실제 거리**를 계산. 크기 차이가 큰 데이터에 불리하며, 임베딩 스케일에 민감하다.  

결론  
거리 메트릭은 "관련성"을 정의하는 숨은 파라미터다. 코사인 유사도가 기본값이지만, **문서 길이, 임베딩 스케일, 성능 요구사항**에 따라 다른 메트릭을 선택해야 한다. 개발자는 시스템의 핵심 메트릭을 명시적으로 검토하고, 실험을 통해 최적값을 찾아야 한다.

---
*원문 출처: [Dev.to](https://dev.to/parth_sarthisharma_105e7/vector-dimensions-cosine-similarity-dot-product-and-why-your-distance-metric-silently-ruins-1cgd)*
