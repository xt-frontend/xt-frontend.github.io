---
layout: post
title: "벡터 차원, 코사인 유사도, 내적 — 왜 당신의 거리 계산법이 유의미함을 묵묵히 파괴하는가"
author: ai-assistant
categories: [AI Dev]
tags: [AI, 개발]
image: assets/images/ai-dev/ai-dev-4-74c30587.jpg
beforetoc: "AI 개발 트렌드 뉴스입니다. 원문: Dev.to"
toc: true
---

semantic search에서 '관련성'을 결정하는 핵심은 임베딩이 아니라 **거리 계산 방식**이다. 대부분의 RAG 시스템이 실패하는 이유는 잘못된 유사도 메트릭 선택에서 비롯된다. 이 글에서 핵심 원리와 개발자에게 필요한 인사이트를 정리해본다.

**벡터 차원의 진짜 의미**  
임베딩 벡터의 차원(예: 768, 1024)은 단어 수가 아니라 **의미의 힌트**를 나타낸다. 더 많은 차원은 미묘한 의미 차이를 포착하지만, 메모리와 계산 비용도 증가시킨다. 단순히 '더 크면 더 좋다'는 오해는 실수로 이어진다. 예를 들어, 챗봇처럼 짧은 문장을 다루는 경우 384차원이 충분할 수 있다.

**3가지 핵심 유사도 메트릭**  
1️⃣ **코사인 유사도**  
- **방향**만 비교(크기 무시) → 길이가 다른 텍스트도 비교 가능  
- 자연어, RAG 시스템에 최적화됨  
- 대부분의 모델이 코사인 기반으로 학습됨  

2️⃣ **도트 프로덕트**  
- **방향 + 크기**를 모두 고려  
- 긴 문서일수록 점수가 높아질 수 있음  
- 문서 길이가 중요한 케이스(예: 긴 FAQ)에 유용  

3️⃣ **유클리드 거리**  
- **벡터 간 실제 거리**를 계산  
- 크기 차이가 큰 데이터에 민감  
- 정규화되지 않은 경우 오류 발생 가능성  

**개발자에게 필요한 인사이트**  
- 벡터 DB의 **기본 유사도 설정**을 점검한다. 코사인 대신 유클리드가 기본값일 수 있다.  
- 모델이 학습된 메트릭과 실제 사용 메트릭이 일치하는지 확인한다.  
- **유저 테스트**로 메트릭을 평가한다. "이 쿼리에 대한 결과가 정말 관련성이 있나?"라는 질문을 끊임없이 던진다.  

결국, 임베딩의 품질 못지않게 **거리 계산 방식의 선택**이 시스템의 정확도를 결정한다. 개발자는 단순히 숫자를 좇기보다, 사용 케이스에 맞는 메트릭을 고민하는 습관을 기를 필요가 있다.

---
*원문 출처: [Dev.to](https://dev.to/parth_sarthisharma_105e7/vector-dimensions-cosine-similarity-dot-product-and-why-your-distance-metric-silently-ruins-1cgd)*
